{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complicated-sapphire",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expected-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-throw",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attached-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, labels_present=True):\n",
    "\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    # TODO - Currently misses first column if uses on data without label column\n",
    "\n",
    "    if labels_present:\n",
    "        \n",
    "        X = data.iloc[:,1:].to_numpy().astype(\"float32\").reshape(-1,28,28)/255.0\n",
    "        print(\"Training images shape:\", X.shape)\n",
    "        \n",
    "        Y = data[\"label\"].to_numpy().astype(\"float32\").reshape(-1,)\n",
    "        print(\"Training labels shape:\", Y.shape)\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        X = data.iloc[:,1:].to_numpy().astype(\"float32\").reshape(-1,28,28)/255.0\n",
    "        print(\"Test images shape:\", X.shape)\n",
    "        \n",
    "        ids = data[\"id\"].to_numpy().astype(\"int\").reshape(-1,)\n",
    "        print(\"IDs shape:\", ids.shape)\n",
    "        \n",
    "        return X, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convertible-bridal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (60000, 28, 28)\n",
      "Training labels shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data/\"\n",
    "\n",
    "X, Y = load_data(data_dir + \"train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ranging-compilation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "greenhouse-light",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "speaking-gender",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\toby\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-sauce",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "\n",
    "Could remove cuda's, as hard to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coupled-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "from torch.autograd import Variable\n",
    "\n",
    "def cuda(v):\n",
    "    if torch.cuda.is_available():\n",
    "        return v.cuda()\n",
    "    return v\n",
    "def toTensor(v,dtype = torch.float,requires_grad = False):       \n",
    "    return cuda(Variable(torch.tensor(v)).type(dtype).requires_grad_(requires_grad))\n",
    "def toNumpy(v):\n",
    "    if torch.cuda.is_available():\n",
    "        return v.detach().cpu().numpy()\n",
    "    return v.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-thompson",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greek-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "# A dataset class that can be used by PyTorch data loaders\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, Y, num_classes=1, aug_transform=None):\n",
    "\n",
    "        self.X = X\n",
    "        # Convert labels to a tensor for training\n",
    "        # self.Y = nn.functional.one_hot(toTensor(Y).long(), num_classes=num_classes)\n",
    "        self.Y = toTensor(Y).long()\n",
    "        \n",
    "        X, Y = None, None\n",
    "\n",
    "        self.xi_preprocessing_transform = transforms.Compose([\n",
    "                                            transforms.ToTensor(),\n",
    "                                          ])\n",
    "        self.yi_preprocessing_transform = None\n",
    "        self.aug_transform = aug_transform\n",
    "        if self.aug_transform is not None:\n",
    "            print(\"[WARNING]: Dataset loaded with augmentations set, make sure this is training only\")\n",
    "        \n",
    "        self.nitems = self.X.shape[0]\n",
    "        \n",
    "        self.xi = None\n",
    "        self.yi = None\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        X, Y = None, None # Avoiding accidently using X and Y instead of self.X\n",
    "\n",
    "        xi = self.X[index]\n",
    "        yi = self.Y[index]\n",
    "        \n",
    "        xi_final = xi\n",
    "        yi_final = yi\n",
    "        \n",
    "        seed = random.randrange(sys.maxsize) #get a random seed so that we can reproducibly do the transformations\n",
    "\n",
    "        # First augment the data, if we have some augmentations to apply\n",
    "        if self.aug_transform is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "            trans_dict = self.aug_transform(image=xi)\n",
    "            # Apply to the images before doing standard transforms\n",
    "            xi = trans_dict[\"image\"]\n",
    "\n",
    "        # Convert examples to a tensor\n",
    "        if self.xi_preprocessing_transform is not None:\n",
    "            random.seed(seed) # apply this seed to img transforms\n",
    "            xi_final = self.xi_preprocessing_transform(xi)\n",
    "\n",
    "        return xi_final,  yi_final\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.nitems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-animation",
   "metadata": {},
   "source": [
    "# Model Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-incentive",
   "metadata": {},
   "source": [
    "## BasicCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "postal-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    # num_classes is the number of COUNTS to return\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.name = \"ConvNet\"\n",
    "\n",
    "        # Layer 1 variables:\n",
    "        layer1_kernel_num = 16\n",
    "        self.layer1 = nn.Sequential(\n",
    "\n",
    "            # Kernel details\n",
    "            nn.Conv2d(in_channels, layer1_kernel_num, kernel_size=5, stride=1, padding=2),\n",
    "\n",
    "            # Normalisation\n",
    "            nn.BatchNorm2d(layer1_kernel_num), # Should match output size of layer\n",
    "\n",
    "            # Activation Function (if any)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Halves the input size\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Layer 2 variables:\n",
    "        layer2_kernel_num = 32\n",
    "        self.layer2 = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(layer1_kernel_num, layer2_kernel_num, kernel_size=5, stride=1, padding=2),\n",
    "\n",
    "            nn.BatchNorm2d(layer2_kernel_num),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Try sequential here\n",
    "        self.fc1 = nn.Linear(7*7*layer2_kernel_num, num_classes)\n",
    "\n",
    "        self.output_layer = nn.Softmax(dim=1)\n",
    "\n",
    "        print(\"Model initialised.\")\n",
    "\n",
    "    # x: numpy array with shape (256,256,3)\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.layer1(x)\n",
    "\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        # Flatten output of Conv part\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "# TODO - predict function\n",
    "# Should start at a raw image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "commercial-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code which goes through the results collected during training and plots them.\n",
    "def plotResults(trainingResults):\n",
    "\n",
    "    metrics_available = trainingResults[list(trainingResults.keys())[0]].keys()\n",
    "\n",
    "    for i, metric in enumerate(metrics_available):\n",
    "        \n",
    "        plt.figure(figsize=(10,5))\n",
    "\n",
    "        plt.title(metric + \" over the course of training\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(metric)\n",
    "        # Values were recorded for train and validation results\n",
    "        for phase in trainingResults.keys():\n",
    "\n",
    "            epochs = len(trainingResults[phase][metric])\n",
    "            epochs_range = range(1,epochs+1)\n",
    "\n",
    "            plt.plot(epochs_range,trainingResults[phase][metric],label=phase)\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "plastic-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct(preds, labels, debug=False):\n",
    "\n",
    "    correct_count = 0.0\n",
    "    \n",
    "    predicted_labels = torch.argmax(preds, dim=1)\n",
    "    correct_count += (predicted_labels == labels).sum().item()\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Predicted:\",toNumpy(predicted_labels))\n",
    "        print(\"Labels:\",toNumpy(labels))\n",
    "        print(\"Correct Count:\",correct_count)\n",
    "    \n",
    "    return correct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "powerful-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_class,model_init_params,Xtr,Ytr,valSplitSize=0.2,batch_size=32,num_epochs=10,lr=1e-3,verbose=0):\n",
    "    \n",
    "    X, Y = None, None\n",
    "    \n",
    "    trainSize = int((1.0 - valSplitSize)*len(Xtr))\n",
    "    print(\"Train set size:\",trainSize)\n",
    "    # Validation set\n",
    "    Xv, Yv = Xtr[trainSize:], Ytr[trainSize:]\n",
    "    # Train set\n",
    "    Xtr, Ytr = Xtr[:trainSize], Ytr[:trainSize]\n",
    "\n",
    "    print(\"Training Dataset sizes:\",\"[\", Xtr.shape, Ytr.shape,\"]\")\n",
    "    print(\"Validation Dataset sizes:\",\"[\", Xv.shape, Yv.shape,\"]\")\n",
    "    \n",
    "    device = torch.device(f'cpu')\n",
    "    \n",
    "    model = model_class(**model_init_params).to(device)\n",
    "    print(\"Model Loaded:\",model.name)\n",
    "    \n",
    "    # Make datasets\n",
    "    train_dataset = CustomDataset(Xtr, Ytr, num_classes=model.num_classes)\n",
    "    val_dataset = CustomDataset(Xv, Yv, num_classes=model.num_classes)\n",
    "\n",
    "    # Make loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                              shuffle=True, num_workers=0) \n",
    "    \n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                              shuffle=True, num_workers=0) \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(\"Loss Function:\",criterion)\n",
    "    \n",
    "        # Uses our learning rate parameter\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    results = dict()\n",
    "    for phase in [\"training\",\"validation\"]:\n",
    "        results[phase] = {\"loss\": [], \"accuracy\": []}\n",
    "    \n",
    "    lowestValidationAcc = np.NINF\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        results_this_epoch = dict()\n",
    "        for phase in [\"training\",\"validation\"]:\n",
    "            results_this_epoch[phase] = {\"loss\": [], \"accuracy\": []}\n",
    "        \n",
    "        # Needed since we switch to .eval() during the validation stage\n",
    "        model.train()\n",
    "        for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            preds = model(images)\n",
    "            \n",
    "            loss = criterion(preds, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            results_this_epoch[\"training\"][\"loss\"].append(loss.item())\n",
    "            results_this_epoch[\"training\"][\"accuracy\"].append(count_correct(preds, labels))\n",
    "            \n",
    "        # Validation\n",
    "        \n",
    "        # Making sure we aren't using dropouts and also aren't trainined the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for i, (images, labels) in enumerate(tqdm(val_loader)):\n",
    "\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # Forward pass\n",
    "                preds = model(images)\n",
    "\n",
    "                loss = criterion(preds, labels)\n",
    "                # No update step\n",
    "\n",
    "                results_this_epoch[\"validation\"][\"loss\"].append(loss.item())\n",
    "                results_this_epoch[\"validation\"][\"accuracy\"].append(count_correct(preds, labels))\n",
    "            \n",
    "        print('Epoch [{}/{}]'.format(epoch+1, num_epochs))\n",
    "\n",
    "        for phase in results_this_epoch.keys():\n",
    "            for metric in results_this_epoch[phase].keys():\n",
    "                if metric == \"loss\":\n",
    "                    results[phase][metric].append(np.mean(results_this_epoch[phase][metric]))\n",
    "                elif metric == \"accuracy\":\n",
    "                    if phase == \"training\":\n",
    "                        total = len(Ytr)\n",
    "                    elif phase == \"validation\":\n",
    "                        total = len(Yv)\n",
    "                    results[phase][metric].append(np.sum(results_this_epoch[phase][metric])/total)\n",
    "                    \n",
    "                if (verbose > 0):\n",
    "                    print('[{}] {}: {:.4f}'.format(phase, metric, results[phase][metric][-1]))\n",
    "                \n",
    "        # Early stopping, preventing overfitting when using too many epochs        \n",
    "        if results[\"validation\"][\"accuracy\"][-1] > lowestValidationAcc:\n",
    "            lowestValidationAcc = results[\"validation\"][\"accuracy\"][-1]\n",
    "            if verbose > 0:\n",
    "                print(\"  **\")\n",
    "            state = {'epoch': epoch + 1,\n",
    "                     'model_dict': model.state_dict(),\n",
    "                     'params': model_init_params,\n",
    "                     'best_loss_on_test': lowestValidationAcc}\n",
    "\n",
    "            modelLocation = model.name + \"_best_model.pth\"\n",
    "            torch.save(state, modelLocation) # saves to current directory\n",
    "            if verbose > 0:\n",
    "                  print(\"Model information (including weights) saved to:\",modelLocation)\n",
    "                    \n",
    "    plotResults(results)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "swiss-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1600\n",
      "Training Dataset sizes: [ (1600, 28, 28) (1600,) ]\n",
      "Validation Dataset sizes: [ (400, 28, 28) (400,) ]\n",
      "Model initialised.\n",
      "Model Loaded: ConvNet\n",
      "Loss Function: CrossEntropyLoss()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  3.21it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "[training] loss: 1.9614\n",
      "[training] accuracy: 0.6219\n",
      "[validation] loss: 2.2110\n",
      "[validation] accuracy: 0.7825\n",
      "  **\n",
      "Model information (including weights) saved to: ConvNet_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.57it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n",
      "[training] loss: 1.5791\n",
      "[training] accuracy: 0.9381\n",
      "[validation] loss: 1.8442\n",
      "[validation] accuracy: 0.8500\n",
      "  **\n",
      "Model information (including weights) saved to: ConvNet_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.37it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n",
      "[training] loss: 1.5185\n",
      "[training] accuracy: 0.9644\n",
      "[validation] loss: 1.6318\n",
      "[validation] accuracy: 0.8625\n",
      "  **\n",
      "Model information (including weights) saved to: ConvNet_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  2.95it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n",
      "[training] loss: 1.5026\n",
      "[training] accuracy: 0.9712\n",
      "[validation] loss: 1.6059\n",
      "[validation] accuracy: 0.8675\n",
      "  **\n",
      "Model information (including weights) saved to: ConvNet_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  2.65it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n",
      "[training] loss: 1.4940\n",
      "[training] accuracy: 0.9744\n",
      "[validation] loss: 1.6089\n",
      "[validation] accuracy: 0.8925\n",
      "  **\n",
      "Model information (including weights) saved to: ConvNet_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:04<00:00,  3.04it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n",
      "[training] loss: 1.4897\n",
      "[training] accuracy: 0.9769\n",
      "[validation] loss: 1.5498\n",
      "[validation] accuracy: 0.8975\n",
      "  **\n",
      "Model information (including weights) saved to: ConvNet_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.71it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n",
      "[training] loss: 1.4861\n",
      "[training] accuracy: 0.9806\n",
      "[validation] loss: 1.5806\n",
      "[validation] accuracy: 0.8950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.46it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n",
      "[training] loss: 1.4830\n",
      "[training] accuracy: 0.9850\n",
      "[validation] loss: 1.5573\n",
      "[validation] accuracy: 0.9150\n",
      "  **\n",
      "Model information (including weights) saved to: ConvNet_best_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.37it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n",
      "[training] loss: 1.4796\n",
      "[training] accuracy: 0.9856\n",
      "[validation] loss: 1.5871\n",
      "[validation] accuracy: 0.9075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3813f48dd639>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConvNet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-ddd9017bc7e5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model_class, model_init_params, Xtr, Ytr, valSplitSize, batch_size, num_epochs, lr, verbose)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;31m# Backward and optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\toby\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\toby\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = train(ConvNet,{},X[:2000],Y[:2000],verbose=1,num_epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_raw(filename, model):\n",
    "    \n",
    "    device = torch.device(f'cpu')\n",
    "\n",
    "    X, ids = load_data(filename, labels_present=False)\n",
    "\n",
    "    dataset = CustomDataset(X, np.zeros((len(X),)), num_classes=model.num_classes)\n",
    "\n",
    "    X = None # Free X\n",
    "\n",
    "    # Make loaders\n",
    "    data_loader = DataLoader(dataset, batch_size=128, shuffle=False, num_workers=0) \n",
    "\n",
    "    predicted_labels = np.asarray([],dtype=\"int\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (images, _) in enumerate(tqdm(data_loader)):\n",
    "\n",
    "            images = images.to(device)\n",
    "            # Forward pass\n",
    "            preds = model(images)\n",
    "            \n",
    "            predicted_labels = np.concatenate( (predicted_labels, toNumpy(torch.argmax(preds, dim=1))) )            \n",
    "            \n",
    "    predicted_labels = predicted_labels.reshape(-1,1)\n",
    "    ids = ids.reshape(-1,1)\n",
    "    \n",
    "    return pd.DataFrame( np.concatenate( (ids, predicted_labels), axis=1 ), columns = [\"id\",\"label\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = infer_raw(data_dir + \"test.csv\", trained_model)\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-receptor",
   "metadata": {},
   "source": [
    "# Tried Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-measure",
   "metadata": {},
   "source": [
    "### Extra Layers\n",
    "\n",
    "More capacity but overfits after a few epochs\n",
    "\n",
    ".9970/0.9980 training scores\n",
    "\n",
    "batch size of 128 or 256\n",
    "\n",
    "lr lowered to 1e-4 seems to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    # num_classes is the number of COUNTS to return\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.name = \"ConvNet\"\n",
    "\n",
    "        # Layer 1 variables:\n",
    "        layer1_kernel_num = 16\n",
    "        self.layer1 = nn.Sequential(\n",
    "\n",
    "            # Kernel details\n",
    "            nn.Conv2d(in_channels, layer1_kernel_num, kernel_size=3, stride=1, padding=1),\n",
    "\n",
    "            # Normalisation\n",
    "            nn.BatchNorm2d(layer1_kernel_num), # Should match output size of layer\n",
    "\n",
    "            # Activation Function (if any)\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Halves the input size\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Layer 2 variables:\n",
    "        layer2_kernel_num = 32\n",
    "        self.layer2 = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(layer1_kernel_num, layer2_kernel_num, kernel_size=5, stride=1, padding=2),\n",
    "\n",
    "            nn.BatchNorm2d(layer2_kernel_num),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        layer3_kernel_num = 64\n",
    "        self.layer3 = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(layer2_kernel_num, layer3_kernel_num, kernel_size=5, stride=1, padding=2),\n",
    "\n",
    "            nn.BatchNorm2d(layer3_kernel_num),\n",
    "\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Try sequential here\n",
    "        self.fc1 = nn.Linear(576, num_classes)\n",
    "\n",
    "        self.output_layer = nn.Softmax(dim=1)\n",
    "\n",
    "        print(\"Model initialised.\")\n",
    "\n",
    "    # x: numpy array with shape (256,256,3)\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.layer1(x)\n",
    "\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "\n",
    "        # Flatten output of Conv part\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "\n",
    "#         print(out.size())\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "# TODO - predict function\n",
    "# Should start at a raw image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "advisory-carbon",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Concatenate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-995b7b9dc025>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCompose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\toby\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\albumentations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"1.3.0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0maugmentations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\toby\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\albumentations\\augmentations\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Common classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mblur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mblur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcrops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcrops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\toby\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\albumentations\\augmentations\\blur\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\toby\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\albumentations\\augmentations\\blur\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvolve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m from albumentations.augmentations.utils import (\n",
      "\u001b[1;32mc:\\users\\toby\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\albumentations\\augmentations\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m from albumentations.augmentations.utils import (\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mMAX_VALUES_BY_DTYPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0m_maybe_process_in_chunks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\toby\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\albumentations\\augmentations\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParamSpec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeypoints_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mangle_to_2pi_range\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Concatenate'"
     ]
    }
   ],
   "source": [
    "from albumentations import Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "australian-resolution",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Concatenate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9fb60d5163e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Concatenate'"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attached-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-patent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
